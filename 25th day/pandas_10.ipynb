{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble\n",
    "\n",
    "# 앙상블은 구성요소가 여러개 존재함 / 머신 러닝과 다른 점이 존재함\n",
    "\n",
    "# Mixing data\n",
    "# 1. k-fold cross validation\n",
    "# 2. Bootstrap Aggregation (bagging)\n",
    "# 3. Random Training Subset\n",
    "\n",
    "# resample - replacement가 있는지 없는지 (꺼내고 다시 집어넣는지 안집어 넣는지)\n",
    "# data가 많으면 복원추출할 필요가 없음\n",
    "# 복원추출의 좋은점: 많이 뽑힌 애들은 중요하다고 생각할 수 있음\n",
    "# 평균에 대해서 한정해야함 / bagging을 사용해서 overfitting 줄일 수 있다\n",
    "# 여러개 만들어서 투표하는 것이므로 성능도 나쁘지 않다\n",
    "\n",
    "# voting?\n",
    "# 같은 data로 다른 알고리즘을 써서 만든 model들\n",
    "# 하지만 하이퍼파라미터가 다르면 다른 algorithm으로 볼 수 있다\n",
    "# 얘는 \"같은\" 데이터로 만든것\n",
    "\n",
    "# hard는 다른 모델들이 예측한 값들의 빈도가 높은 애들이 곧 결과값으로 나온다\n",
    "# 문제는 다수의 횡포로 틀린 답을 낼 수도 있다\n",
    "\n",
    "# Soft(Averaging)\n",
    "# 확률만 더해서 - 투표권을 모델당 하나의 투표권을 줄수도 있고, 모델별로 가중치를 둘 수도 있다\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_predict(KNeighborsClassifier(), iris.iloc[:,:-1], iris.target, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 내부적인것, K-fold 존재함\n",
    "# kfold를 split 할 수 있음\n",
    "\n",
    "# cv = KFold()\n",
    "# cv.split()\n",
    "\n",
    "# generator로서 나온다\n",
    "# data를 next로 받고\n",
    "# 모델을 만든다\n",
    "\n",
    "# Kfold에는 여러가지 방법이 존재한다\n",
    "# ex) stratifiedKFold - y에 맞춰서...\n",
    "\n",
    "# Model = Algorithm + Data\n",
    "# 즉, data가 다르면 다른 model이다\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bagging 구현\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(iris.iloc[:,:-1], iris.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc = VotingClassifier([('knn', KNeighborsClassifier()), ('lr', LogistricRegression())])\n",
    "# sklearn에서 estimator는 algorithm들이다\n",
    "# n-jobs는 알고리즘과 상관없이 joblib (병렬계산) / n_jobs 2면 CPU 2코어로 계산할 것이다\n",
    "vc.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가중치를 어떻게 주느냐는 어렵다\n",
    "from sklearn.ensemble import BaggingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb = BaggingClassifier()\n",
    "# estimator가 default는 decision tree다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb.fit(X_train, y_train)\n",
    "# 이렇게하면 모델 만드는 것 끝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest = bagging + decision tree\n",
    "# bagging = 똑같은 dataset에서 다른 dataset을 구성하고 다른 알고리즘으로 구성해서 만드는 것\n",
    "# 알고리즘을 다르겧는 방식들\n",
    "\n",
    "# Hyperparameter Tuning - learning data, 초기값 바꾸는 것 - 다른 형태의 data가 나온다\n",
    "\n",
    "# snapshot - epoch의 수를 여러가지 나눠서 더하는 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boosting\n",
    "\n",
    "# bagging과 함께 양대산맥\n",
    "# 원본 데이터가 있음 - 얘로 모델 만들었음\n",
    "# 테스트하면 re-substitution testing 함\n",
    "# 그러면 잘 분류하는 것과 잘 못분류하는 것을 알 수 있다\n",
    "# boosting의 핵심은 못하는 애들만 가중치를 주는 것\n",
    "# 그렇게 되면...\n",
    "# 원래 잘 분류하던 애들은 계속 잘 분류하고\n",
    "# 원래 잘 분류가 안되던 애들은 거의 다 잘 분류하는 애들이 될 것이다\n",
    "# 기존 것들도 같이 사용함 왜냐하면 overfitting 날 가능성이 존재하므로\n",
    "\n",
    "# adaboosting\n",
    "# 최초로 인기를 끌었음\n",
    "# 학습시키고나서 잘하는지 아닌지\n",
    "# 그리고 잘 안되는 애들에게는 가중치를 붙여줌\n",
    "# 이 외에도 다른 boosting 기법들이 존재한다\n",
    "# boosting의 장점:\n",
    "# 1. fast learning\n",
    "# 2. 처음에 복잡없이 시작해서 끝이 이쁘게 나온다\n",
    "# 단점:\n",
    "# 1. overfitting의 문제점\n",
    "# 2. binary classification에서만 가능하다\n",
    "\n",
    "# Stacking\n",
    "# NN에서 아이디어 가지고 옴\n",
    "# NN은 아님\n",
    "# Perceptron 대신 다른 방식으로 계산하는 것이 stacking\n",
    "# 정형 데이터가 많기 때문이다\n",
    "# 모델 나온 가진 결과를 가지고 새로운 모델을 만드는 것이 stacking\n",
    "\n",
    "# cross validation 새로운 모델 만들때 학습되게 함\n",
    "# stacking에서\n",
    "# 학습이 잘 안되가지고 blending이 나옴\n",
    "\n",
    "# blending\n",
    "# 3-way hold out (cross validation 대신에...)\n",
    "# 이런 기법들은 다 사용함\n",
    "\n",
    "# 따라서, 전처리가 중요한데 이는 insight가 중요함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StackingClassifier\n",
    "# final estimator = logistic regression"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KoreaUniv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
