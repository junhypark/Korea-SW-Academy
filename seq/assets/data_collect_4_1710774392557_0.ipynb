{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selector 사용 추천 왜? 활용 방법이 많기 때문이다\n",
    "# 지칭, 구조, 관계 - 가상 선택자 혹은 형제, 자식, 부모 계열을 찾아가는 것\n",
    "# pseudo selector를 확인하면 selector 확인이 가능하다\n",
    "# 태그이름, #아이디, .클래스, .클래스.클래스, 태그이름#아이디, #아이디.클래스\n",
    "# (주의) 띄어쓰기 다른 의미, 띄어쓰기는 자손을 의미함\n",
    "# 자식 >, 다음 형제 +, 다음 형제노드들 ~\n",
    "\n",
    "# ex) span + a ==> a를 찾아야하는데 이전에 span이라는 얘가 있었어야함\n",
    "# <p>\n",
    "#     <div></div>\n",
    "#     <span></span><a></a>\n",
    "# </p>\n",
    "# 이렇게 되어 있으면 span 의 a 를 찾을 수 있다\n",
    "\n",
    "# ex) span ~ a\n",
    "#<p>\n",
    "#     <div><a></a></div>    ... 1\n",
    "#     <span></span><a></a><a></a><a></a><a></a><a></a><a></a>   ... 2\n",
    "# </p>\n",
    "# 1번 거르고 2번만 가지고 온다\n",
    "\n",
    "# 가상선택자\n",
    "# :has, :is, :nth-child(같은 부모를 공유하는 자식들의 순선), :nth-of-type(같은 종류의 태그 중 순번)\n",
    "# <a class=\"A\">\n",
    "# <a class=\"B\">...\n",
    "# ex) a:is(.A) class A를 가진 a태그 찾기\n",
    "\n",
    "# Crawler - link를 따라 다니면서 웹페이지에 무슨 내용이 있었는지 수집(index/indexing) + 다음 link를 찾아서 또 따라다니는 역할\n",
    "# Tree (bfs, dfs) - search space를 탐색 후 계산 가능한 범위로 줄여야 하기 때문이다\n",
    "# link를 어떻게 찾을 것인가? => link 생긴게 지 멋대로 생겨있음 (ex)mailto:...) => 따라서 정규화 실행 => 다음번 방문할 link (방문한곳, 방문할 곳에 포함되지 않은 link)\n",
    "# 이것을 focused crawling으로 link 방문을 제한시켜서 사용 가능하게끔 만듦\n",
    "# => focused crawling + scraping + DB\n",
    "# => 이때 도는 link를 우선순위로 만듦 Citation(href:linking) => PageRank 라는 알고리즘으로 Citation 우선순위를 매길 수 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from requests.compat import urlparse, urljoin\n",
    "from requests import get\n",
    "import re\n",
    "\n",
    "def canFetch(url, path):\n",
    "    resp = get(urljoin(url, '/robots.txt')) # 특정 사이트 robot.txt 접근\n",
    "    if resp.status_code != 200: # 200 OK만 나오면 괜찮\n",
    "        return True\n",
    "    \n",
    "    disallowList = [link for link in re.findall(r'disallow\\s*:\\s*(.+)', resp.text, re.IGNORECASE)]\n",
    "    # 대문자 소문자 무시하고, resp.text 중에서 disallow 뒤에 있는 애들을 link라는 for문으로 list 작성\n",
    "    \n",
    "    if urlparse(path).path in disallowList:\n",
    "        return False\n",
    "    # 만약 path가 disallowList(즉, 가면 안되는 곳)에 있으면 False 반환\n",
    "\n",
    "    return True # 나머지는 True 반환\n",
    "\n",
    "canFetch('https://www.google.com', '/search?q=수지')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import request\n",
    "from requests.exceptions import HTTPError\n",
    "from time import sleep\n",
    "\n",
    "def download(url, params={}, data={}, headers={}, method = 'GET', retries=3):\n",
    "    # if not canFetch(url, url):  # robots.txt 검사\n",
    "    #     print('수집하면 안됨')  # False일때\n",
    "\n",
    "    resp = request(method, url, params=params, data=data, headers=headers)\n",
    "    # parameter로 받은 친구들을 request 옵션으로 넣어서 request 객체 생성\n",
    "    try:\n",
    "        resp.raise_for_status() # 에러 확인 용도, HTTPError 뜨면 except로 동작\n",
    "    except HTTPError as e:\n",
    "        if 499 < resp.status_code and retries > 0:  # 500번 에러 그리고 retry가 0이 아직 아닐때\n",
    "            print('재시도 중')                       # 계속 돌아야 하기 때문임\n",
    "            sleep(5)                                # traffic 너무 먹으면 안되므로\n",
    "            return download(url, params, data, headers, method, retries-1)  # retry 한번 했으니 하나 빼고\n",
    "        else:\n",
    "            print(e.response.status_code)\n",
    "            print(e.request.headers)\n",
    "            print(e.response.headers)\n",
    "            # 에러들 보여주기\n",
    "            return None\n",
    "    \n",
    "    return resp\n",
    "    # 마지막으로 request 객체 반환\n",
    "# download('https://httpbin.org/status/500')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "URLs = []\n",
    "Visited = []\n",
    "ua = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36'\n",
    "\n",
    "# URLs.appen('https://search.naver.com/search.naver?where=nexearch&sm=top_hty&fbm=0&ie=utf8&query=수지')\n",
    "URLs.append('https://www.google.com/search?q=수지')\n",
    "\n",
    "while URLs:\n",
    "    url = URLs.pop(0)   # 0: FIFO; Queue / -1: FILO; Stack ==> url = URLs.pop(-1)\n",
    "\n",
    "    Visited.append(url)\n",
    "\n",
    "    resp = download(url, headers={'user-agent': ua})\n",
    "    \n",
    "    if resp is None:\n",
    "        continue\n",
    "\n",
    "    if re.search(r'text\\/html', resp.headers['content-type']):\n",
    "        dom = BeautifulSoup(resp.text, 'html.parser')\n",
    "\n",
    "        for link in dom.select('a[href], iframe[src], img[src], audio[src], frame[src], video[src], style[src], link[src]'):\n",
    "            newURL = link.attrs['href'] if link.has_attr('href') else link.attrs['src']\n",
    "\n",
    "            if not re.match(r'#|javascript|mailto|data', newURL):\n",
    "                noremlizedURL = urljoin(url, newURL)\n",
    "                if noremlizedURL not in Visited and noremlizedURL not in URLs:\n",
    "                    URLs.append(noremlizedURL)\n",
    "\n",
    "    # print(len(URLs), len(Visited))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# focused crawling 만들어보자\n",
    "# 내 관심사에만 집중하자\n",
    "# 전략 존재\n",
    "# 1. depth제한 - depth가 깊어질수록 원래 관심사에서 멀어지는 경향이 존재하기 때문이다\n",
    "# 2. domian 제한 - 볼 필요 없는 블로그 혹은 카페 이런 것들을 제외시키는 것\n",
    "# 3. HTML tags 제한\n",
    "\n",
    "URLs = []\n",
    "Visited = []\n",
    "ua = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36'\n",
    "\n",
    "URLs.append({'link':'https://search.naver.com/search.naver?where=nexearch&sm=top_hty&fbm=0&ie=utf8&query=수지', 'depth': 0}) # fix\n",
    "# URLs.append('https://www.google.com/search?q=수지')\n",
    "\n",
    "while URLs:\n",
    "    url = URLs.pop(0)   # 0: FIFO; Queue / -1: FILO; Stack ==> url = URLs.pop(-1)\n",
    "\n",
    "    Visited.append(url['link']) # fix\n",
    "\n",
    "    resp = download(url['link'], headers={'user-agent': ua}) # fix\n",
    "    \n",
    "    if resp is None:\n",
    "        continue\n",
    "\n",
    "    if re.search(r'text\\/html', resp.headers['content-type']):\n",
    "        dom = BeautifulSoup(resp.text, 'html.parser')\n",
    "\n",
    "        for link in dom.select('a[href], iframe[src], img[src], audio[src], frame[src], video[src], style[src], link[src]'):\n",
    "            newURL = link.attrs['href'] if link.has_attr('href') else link.attrs['src']\n",
    "\n",
    "            if not re.match(r'#|javascript|mailto|data', newURL):\n",
    "                noremlizedURL = urljoin(url['link'], newURL)            # fix\n",
    "                if noremlizedURL not in Visited and noremlizedURL not in URLs:\n",
    "                    URLs.append({'link':noremlizedURL, 'depth': url['depth']+1}) # fix\n",
    "\n",
    "    # print(len(URLs), len(Visited))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "629 13\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None,\n",
       " 'https://dict.naver.com/dict.search?query=%EC%88%98%EC%A7%80&from=tsearch',\n",
       " {'link': 'https://www.navercorp.com/', 'depth': 2})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(URLs), len(Visited)), Visited[-1], URLs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack 변환\n",
    "# 원래 정보에서 굉장히 많이 떨어진 페이지가 나온다\n",
    "\n",
    "URLs = []\n",
    "Visited = []\n",
    "ua = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36'\n",
    "\n",
    "URLs.append({'link':'https://search.naver.com/search.naver?where=nexearch&sm=top_hty&fbm=0&ie=utf8&query=수지', 'depth': 0}) # fix\n",
    "# URLs.append('https://www.google.com/search?q=수지')\n",
    "\n",
    "while URLs:\n",
    "    url = URLs.pop(-1)   # 0: FIFO; Queue / -1: FILO; Stack ==> url = URLs.pop(-1)\n",
    "\n",
    "    Visited.append(url['link']) # fix\n",
    "\n",
    "    resp = download(url['link'], headers={'user-agent': ua}) # fix\n",
    "    \n",
    "    if resp is None:\n",
    "        continue\n",
    "\n",
    "    if re.search(r'text\\/html', resp.headers['content-type']):\n",
    "        dom = BeautifulSoup(resp.text, 'html.parser')\n",
    "\n",
    "        for link in dom.select('a[href], iframe[src], img[src], audio[src], frame[src], video[src], style[src], link[src]'):\n",
    "            newURL = link.attrs['href'] if link.has_attr('href') else link.attrs['src']\n",
    "\n",
    "            if not re.match(r'#|javascript|mailto|data', newURL):\n",
    "                noremlizedURL = urljoin(url['link'], newURL)            # fix\n",
    "                if noremlizedURL not in Visited and noremlizedURL not in URLs:\n",
    "                    URLs.append({'link':noremlizedURL, 'depth': url['depth']+1}) # fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "904 8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None,\n",
       " 'https://www.youtube.com/t/terms?archive=20191210',\n",
       " {'link': 'https://support.google.com/accounts/contact/suspended?p=youtube',\n",
       "  'depth': 7})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(URLs), len(Visited)), Visited[-1], URLs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URLs = []\n",
    "Visited = []\n",
    "Skipped = []    # fix\n",
    "ua = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36'\n",
    "\n",
    "depth = 3   # 제한할 depth  # fix\n",
    "\n",
    "URLs.append({'link':'https://search.naver.com/search.naver?where=nexearch&sm=top_hty&fbm=0&ie=utf8&query=수지', 'depth': 0}) # fix\n",
    "# URLs.append('https://www.google.com/search?q=수지')\n",
    "\n",
    "while URLs:\n",
    "    url = URLs.pop(-1)   # 0: FIFO; Queue / -1: FILO; Stack ==> url = URLs.pop(-1)\n",
    "    \n",
    "    if url['depth'] > depth:    # 제한 1. 깊이로 제한\n",
    "        Skipped.append(url['link']) # fix\n",
    "        continue\n",
    "\n",
    "    Visited.append(url['link'])\n",
    "\n",
    "    resp = download(url['link'], headers={'user-agent': ua}) \n",
    "    \n",
    "    if resp is None:\n",
    "        continue\n",
    "\n",
    "    if re.search(r'text\\/html', resp.headers['content-type']):\n",
    "        dom = BeautifulSoup(resp.text, 'html.parser')\n",
    "\n",
    "        for link in dom.select('a[href], iframe[src], img[src], audio[src], frame[src], video[src], style[src], link[src]'):\n",
    "            newURL = link.attrs['href'] if link.has_attr('href') else link.attrs['src']\n",
    "\n",
    "            if not re.match(r'#|javascript|mailto|data', newURL):\n",
    "                noremlizedURL = urljoin(url['link'], newURL)            \n",
    "                if noremlizedURL not in Visited and noremlizedURL not in URLs and noremlizedURL not in Skipped: # fix\n",
    "                    URLs.append({'link':noremlizedURL, 'depth': url['depth']+1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "657 9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None,\n",
       " 279,\n",
       " 'https://www.youtube.com/t/privacy',\n",
       " {'link': 'https://www.youtube.com/t/terms', 'depth': 3},\n",
       " 'https://www.youtube.com/howyoutubeworks/static/images/eyebrow-underline.png')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(URLs), len(Visited)), len(Skipped), Visited[-1], URLs[-1], Skipped[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 너비 우선 탐색은 메모리를 많이 먹음\n",
    "# 깊이 우선 탐색은 모든걸 기록해놔야함\n",
    "# space complexity가 높아질수 밖에 없다\n",
    "\n",
    "from requests.compat import urlparse\n",
    "\n",
    "# 2. 도메인 제한\n",
    "URLs = []\n",
    "Visited = []\n",
    "Skipped = []\n",
    "\n",
    "optin = ['v.daum.net']  # 여기 있는 것들만\n",
    "optout = [] # 여기 제외\n",
    "\n",
    "ua = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36'\n",
    "\n",
    "depth = 3   # 제한할 depth  # fix\n",
    "\n",
    "# URLs.append({'link':'https://search.naver.com/search.naver?where=nexearch&sm=top_hty&fbm=0&ie=utf8&query=수지', 'depth': 0})\n",
    "# URLs.append('https://www.google.com/search?q=수지')\n",
    "URLs.append({'link':'https://search.daum.net/search?w=tot&DA=YZR&t__nil_searchbox=btn&q=수지', 'depth': 0}) # fix\n",
    "\n",
    "while URLs:\n",
    "    url = URLs.pop(-1)   # 0: FIFO; Queue / -1: FILO; Stack ==> url = URLs.pop(-1)\n",
    "    \n",
    "    if url['depth'] > depth:    # 제한 1. 깊이로 제한\n",
    "        Skipped.append(url['link']) # fix\n",
    "        continue\n",
    "\n",
    "    Visited.append(url['link'])\n",
    "\n",
    "    resp = download(url['link'], headers={'user-agent': ua}) \n",
    "    \n",
    "    if resp is None:\n",
    "        continue\n",
    "\n",
    "    if re.search(r'text\\/html', resp.headers['content-type']):\n",
    "        dom = BeautifulSoup(resp.text, 'html.parser')\n",
    "\n",
    "        for link in dom.select('a[href], iframe[src], img[src], audio[src], frame[src], video[src], style[src], link[src]'):\n",
    "            newURL = link.attrs['href'] if link.has_attr('href') else link.attrs['src']\n",
    "\n",
    "            if not re.match(r'#|javascript|mailto|data', newURL):\n",
    "                noremlizedURL = urljoin(url['link'], newURL)\n",
    "                # 2. 도메인 제한 -optin\n",
    "                # if urlparse(noremlizedURL).netloc not in optin:\n",
    "                #     continue\n",
    "                # 해당 패스하는 애들만 아래에 저장 \n",
    "                \n",
    "                # 3. 도메인 제한 -optout\n",
    "                # if urlparse(noremlizedURL).netloc in optout:\n",
    "                #     continue\n",
    "                # 해당 맞는 애들은 거르기\n",
    "                if noremlizedURL not in Visited and noremlizedURL not in URLs and noremlizedURL not in Skipped: # fix\n",
    "                    URLs.append({'link':noremlizedURL, 'depth': url['depth']+1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mlen\u001b[39m(URLs), \u001b[38;5;28mlen\u001b[39m(Visited), \u001b[38;5;28mlen\u001b[39m(Skipped), Visited[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], URLs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], \u001b[43mSkipped\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "print(len(URLs), len(Visited), len(Skipped)), Visited[-1], URLs[-1], Skipped[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# naver blog만 해보자\n",
    "# iframe - dom을 dom 안에 넣는 방식\n",
    "\n",
    "URLs = []\n",
    "Visited = []\n",
    "Skipped = []\n",
    "\n",
    "optin = ['blog.naver.com']  # 여기 있는 것들만\n",
    "optout = [] # 여기 제외\n",
    "\n",
    "ua = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36'\n",
    "\n",
    "depth = 3   # 제한할 depth  # fix\n",
    "\n",
    "URLs.append({'link':'https://search.naver.com/search.naver?where=nexearch&sm=top_hty&fbm=0&ie=utf8&query=수지', 'depth': 0})\n",
    "# URLs.append('https://www.google.com/search?q=수지')\n",
    "# URLs.append({'link':'https://search.daum.net/search?w=tot&DA=YZR&t__nil_searchbox=btn&q=수지', 'depth': 0}) # fix\n",
    "\n",
    "while URLs:\n",
    "    url = URLs.pop(-1)   # 0: FIFO; Queue / -1: FILO; Stack ==> url = URLs.pop(-1)\n",
    "    \n",
    "    if url['depth'] > depth:    # 제한 1. 깊이로 제한\n",
    "        Skipped.append(url['link']) # fix\n",
    "        continue\n",
    "\n",
    "    Visited.append(url['link'])\n",
    "\n",
    "    resp = download(url['link'], headers={'user-agent': ua}) \n",
    "    \n",
    "    if resp is None:\n",
    "        continue\n",
    "\n",
    "    if re.search(r'text\\/html', resp.headers['content-type']):\n",
    "        dom = BeautifulSoup(resp.text, 'html.parser')\n",
    "\n",
    "        for link in dom.select('a[href], iframe[src], img[src], audio[src], frame[src], video[src], style[src], link[src]'):\n",
    "            newURL = link.attrs['href'] if link.has_attr('href') else link.attrs['src']\n",
    "\n",
    "            if not re.match(r'#|javascript|mailto|data', newURL):\n",
    "                noremlizedURL = urljoin(url['link'], newURL)\n",
    "                # 2. 도메인 제한 -optin\n",
    "                if urlparse(noremlizedURL).netloc not in optin:\n",
    "                    continue\n",
    "                # 해당 패스하는 애들만 아래에 저장 \n",
    "                \n",
    "                # 3. 도메인 제한 -optout\n",
    "                # if urlparse(noremlizedURL).netloc in optout:\n",
    "                #     continue\n",
    "                # 해당 맞는 애들은 거르기\n",
    "                if noremlizedURL not in Visited and noremlizedURL not in URLs and noremlizedURL not in Skipped: # fix\n",
    "                    URLs.append({'link':noremlizedURL, 'depth': url['depth']+1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 8 108\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None,\n",
       " 'https://blog.naver.com/PostList.naver?blogId=travis88&categoryNo=4&from=postList&parentCategoryNo=0',\n",
       " {'link': 'https://blog.naver.com/travis88/223305078411', 'depth': 3},\n",
       " 'https://blog.naver.com')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(URLs), len(Visited), len(Skipped)), Visited[-1], URLs[-1], Skipped[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://search.naver.com/search.naver?where=nexearch&sm=top_hty&fbm=0&ie=utf8&query=수지',\n",
       " 'https://blog.naver.com/travis88/223325700661',\n",
       " 'https://blog.naver.com/PostView.naver?blogId=travis88&logNo=223325700661&redirect=Dlog&widgetTypeCall=true&directAccess=false',\n",
       " 'https://blog.naver.com/FILEPATH',\n",
       " 'https://blog.naver.com/FILEPATH',\n",
       " 'https://blog.naver.com/connect/WidgetView.naver?blogId=travis88&widgetSeq=1&authCode=be74f3a542918ab9c904c5354b8a691a6957adf',\n",
       " 'https://blog.naver.com/PostList.naver?blogId=travis88&categoryNo=4&from=postList&parentCategoryNo=0',\n",
       " 'https://blog.naver.com/PostList.naver?blogId=travis88&categoryNo=4&from=postList&parentCategoryNo=0']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Visited     # 방문한 블로그들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optin, optout 둘 다 더 추가해서 제한 가능하다\n",
    "# 경로를 제한 가능하다는 뜻\n",
    "# 이번엔 뉴스 링크만\n",
    "\n",
    "URLs = []\n",
    "Visited = []\n",
    "Skipped = []\n",
    "\n",
    "optin = ['news.naver.com', 'm.news.naver.com']  # news만\n",
    "optout = []\n",
    "\n",
    "ua = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36'\n",
    "\n",
    "depth = 5   \n",
    "\n",
    "# URLs.append({'link':'https://search.naver.com/search.naver?where=nexearch&sm=top_hty&fbm=0&ie=utf8&query=수지', 'depth': 0})\n",
    "# URLs.append('https://www.google.com/search?q=수지')\n",
    "# URLs.append({'link':'https://search.daum.net/search?w=tot&DA=YZR&t__nil_searchbox=btn&q=수지', 'depth': 0}) # fix\n",
    "URLs.append({'link':'https://news.naver.com/', 'depth': 0})\n",
    "\n",
    "while URLs:\n",
    "    url = URLs.pop(0) \n",
    "    \n",
    "    if url['depth'] > depth: \n",
    "        Skipped.append(url['link'])\n",
    "        continue\n",
    "\n",
    "    Visited.append(url['link'])\n",
    "\n",
    "    resp = download(url['link'], headers={'user-agent': ua}) \n",
    "    \n",
    "    if resp is None:\n",
    "        continue\n",
    "\n",
    "    if re.search(r'text\\/html', resp.headers['content-type']):\n",
    "        dom = BeautifulSoup(resp.text, 'html.parser')\n",
    "        # 3. 영역제한 - 뉴스만\n",
    "\n",
    "        # 뉴스인 경우\n",
    "        # 3-1. 메뉴\n",
    "        linksA = dom.select('ul.nNlnb_menu_list li > a') # ul 클래스 가진 li의 자식인 a를 찾음  # fix\n",
    "        # 위의 부모가 .sa|ss_text > a:has(> strong) sa혹은 ss_text 클래스가 a를 가지는데 이 a는 strong을 가진다\n",
    "        # 3-2.뉴스\n",
    "        linksB = dom.select('div[class$=\"_text\"] > a[href]:has(>strong)')   # fix\n",
    "\n",
    "        for link in linksA+linksB:  # fix\n",
    "            newURL = link.attrs['href'] if link.has_attr('href') else link.attrs['src']\n",
    "\n",
    "            if not re.match(r'#|javascript|mailto|data', newURL):\n",
    "                noremlizedURL = urljoin(url['link'], newURL)\n",
    "                if urlparse(noremlizedURL).netloc not in optin:\n",
    "                    continue\n",
    "                \n",
    "                # if urlparse(noremlizedURL).netloc in optout:\n",
    "                #     continue\n",
    "                if noremlizedURL not in Visited and noremlizedURL not in URLs and noremlizedURL not in Skipped: # fix\n",
    "                    URLs.append({'link':noremlizedURL, 'depth': url['depth']+1})\n",
    "\n",
    "# 맨처음에 menu만 들어가므로 linksB가 비어있을 것임\n",
    "# 그 후 어딘가에 섹션에 들어가면 linksB가 나올 것이다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1, 0, 'https://news.naver.com/')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(URLs), len(Visited), len(Skipped), Visited[-1], #URLs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 따라서, focused crawling은 뚜렷한 목적에 따라서 수집은 하지만 고정된 것은 아니다\n",
    "# scraping도 섞자\n",
    "from os import mkdir, listdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data_collect_4.ipynb']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listdir('.')\n",
    "# 현재 작업중인 폴더의 파일들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir('./naver_news')\n",
    "# 생성 완료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[65], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m     Skipped\u001b[38;5;241m.\u001b[39mappend(url[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlink\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlink\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43muser-agent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mua\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m Visited\u001b[38;5;241m.\u001b[39mappend(url[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlink\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resp \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[7], line 9\u001b[0m, in \u001b[0;36mdownload\u001b[1;34m(url, params, data, headers, method, retries)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdownload\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m{}, data\u001b[38;5;241m=\u001b[39m{}, headers\u001b[38;5;241m=\u001b[39m{}, method \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGET\u001b[39m\u001b[38;5;124m'\u001b[39m, retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m):\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# if not canFetch(url, url):  # robots.txt 검사\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m#     print('수집하면 안됨')  # False일때\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# parameter로 받은 친구들을 request 옵션으로 넣어서 request 객체 생성\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\SAMSUNG\\anaconda3\\envs\\KoreaUniv\\lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\SAMSUNG\\anaconda3\\envs\\KoreaUniv\\lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\SAMSUNG\\anaconda3\\envs\\KoreaUniv\\lib\\site-packages\\requests\\sessions.py:747\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    744\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    746\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n\u001b[1;32m--> 747\u001b[0m     \u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\n\u001b[0;32m    749\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "File \u001b[1;32mc:\\Users\\SAMSUNG\\anaconda3\\envs\\KoreaUniv\\lib\\site-packages\\requests\\models.py:899\u001b[0m, in \u001b[0;36mResponse.content\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    898\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 899\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCONTENT_CHUNK_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    901\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content_consumed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    902\u001b[0m \u001b[38;5;66;03m# don't need to release the connection; that's been handled by urllib3\u001b[39;00m\n\u001b[0;32m    903\u001b[0m \u001b[38;5;66;03m# since we exhausted the data.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\SAMSUNG\\anaconda3\\envs\\KoreaUniv\\lib\\site-packages\\requests\\models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    814\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    815\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 816\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    817\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    818\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[1;32mc:\\Users\\SAMSUNG\\anaconda3\\envs\\KoreaUniv\\lib\\site-packages\\urllib3\\response.py:931\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m    915\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    916\u001b[0m \u001b[38;5;124;03mA generator wrapper for the read() method. A call will block until\u001b[39;00m\n\u001b[0;32m    917\u001b[0m \u001b[38;5;124;03m``amt`` bytes have been read from the connection or until the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    928\u001b[0m \u001b[38;5;124;03m    'content-encoding' header.\u001b[39;00m\n\u001b[0;32m    929\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    930\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunked \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupports_chunked_reads():\n\u001b[1;32m--> 931\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_chunked(amt, decode_content\u001b[38;5;241m=\u001b[39mdecode_content)\n\u001b[0;32m    932\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    933\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\SAMSUNG\\anaconda3\\envs\\KoreaUniv\\lib\\site-packages\\urllib3\\response.py:1071\u001b[0m, in \u001b[0;36mHTTPResponse.read_chunked\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m   1068\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1070\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m-> 1071\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_chunk_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1072\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1073\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\SAMSUNG\\anaconda3\\envs\\KoreaUniv\\lib\\site-packages\\urllib3\\response.py:999\u001b[0m, in \u001b[0;36mHTTPResponse._update_chunk_length\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    997\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    998\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 999\u001b[0m line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[0;32m   1000\u001b[0m line \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1001\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\SAMSUNG\\anaconda3\\envs\\KoreaUniv\\lib\\socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    703\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 704\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    706\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\SAMSUNG\\anaconda3\\envs\\KoreaUniv\\lib\\ssl.py:1275\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1271\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1272\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1273\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1274\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1275\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1276\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mc:\\Users\\SAMSUNG\\anaconda3\\envs\\KoreaUniv\\lib\\ssl.py:1133\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1132\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1133\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1134\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1135\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# news 본문만 저장\n",
    "\n",
    "URLs = []\n",
    "Visited = []\n",
    "Skipped = []\n",
    "\n",
    "optin = ['news.naver.com', 'n.news.naver.com']  # news만\n",
    "optout = []\n",
    "\n",
    "ua = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36'\n",
    "\n",
    "depth = 5   \n",
    "\n",
    "# URLs.append({'link':'https://search.naver.com/search.naver?where=nexearch&sm=top_hty&fbm=0&ie=utf8&query=수지', 'depth': 0})\n",
    "# URLs.append('https://www.google.com/search?q=수지')\n",
    "# URLs.append({'link':'https://search.daum.net/search?w=tot&DA=YZR&t__nil_searchbox=btn&q=수지', 'depth': 0}) # fix\n",
    "URLs.append({'link':'https://news.naver.com/', 'depth': 0})\n",
    "\n",
    "NEWS_TEXT = './naver_news'\n",
    "NEWS_IMG = './naver_img'\n",
    "\n",
    "while URLs:\n",
    "    url = URLs.pop(-1) \n",
    "    \n",
    "    if url['depth'] > depth: \n",
    "        Skipped.append(url['link'])\n",
    "        continue\n",
    "\n",
    "    resp = download(url['link'], headers={'user-agent': ua})\n",
    "\n",
    "    Visited.append(url['link'])\n",
    "    \n",
    "    if resp is None:\n",
    "        continue\n",
    "\n",
    "    if re.search(r'text\\/html', resp.headers['content-type']):\n",
    "        dom = BeautifulSoup(resp.text, 'html.parser')\n",
    "        # 3. 영역제한 - 뉴스만\n",
    "\n",
    "        # 뉴스인 경우\n",
    "        # 3-1. 메뉴\n",
    "        linksA = dom.select('ul.Nlnb_menu_list li > a') # ul 클래스 가진 li의 자식인 a를 찾음  # fix\n",
    "        # 위의 부모가 .sa|ss_text > a:has(> strong) sa혹은 ss_text 클래스가 a를 가지는데 이 a는 strong을 가진다\n",
    "        # 3-2.뉴스\n",
    "        linksB = dom.select('div[class$=\"_text\"] > a[href]:has(> strong)')   # fix\n",
    "\n",
    "        # 3-3. 뉴스 본문\n",
    "        linksC = dom.select('#dic_area a[href]')\n",
    "        article = dom.select_one('#dic_area')\n",
    "        # 뉴스 본문을 찾기 위해 특정 element만\n",
    "\n",
    "        if article:\n",
    "            with open(NEWS_TEXT+'/'+re.search(r'(\\d{10})$',url['link']).group(1)+'.txt', 'w', encoding='utf8') as fp:  # 기사 number로 text 파일 이름 줄것임\n",
    "                fp.write(article.get_text())\n",
    "        # 저장 file\n",
    "\n",
    "        for link in linksA+linksB+linksC:  # fix\n",
    "            newURL = link.attrs['href'] if link.has_attr('href') else link.attrs['src']\n",
    "\n",
    "            if not re.match(r'#|javascript|mailto|data', newURL):\n",
    "                noremlizedURL = urljoin(url['link'], newURL)\n",
    "                if urlparse(noremlizedURL).netloc not in optin:\n",
    "                    continue\n",
    "                \n",
    "                # if urlparse(noremlizedURL).netloc in optout:\n",
    "                #     continue\n",
    "                if noremlizedURL not in Visited and noremlizedURL not in URLs and noremlizedURL not in Skipped: # fix\n",
    "                    # fix url['link'] for url in URLs\n",
    "                    # why? dict인데 item 안에 들어있으니 in으로 체크하면 무조건 True가 나올 것임\n",
    "                    URLs.append({'link':noremlizedURL, 'depth': url['depth']+1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://news.naver.com/',\n",
       " 'https://news.naver.com/main/ombudsman/errorArticleList.naver?mid=omb',\n",
       " 'https://news.naver.com/factcheck/main',\n",
       " 'https://news.naver.com/main/tv/index.naver?mid=tvh',\n",
       " 'https://news.naver.com/opinion/home',\n",
       " 'https://news.naver.com/newspaper/home?viewType=pc',\n",
       " 'https://news.naver.com/main/ranking/popularDay.naver',\n",
       " 'https://news.naver.com/section/104',\n",
       " 'https://n.news.naver.com/mnews/article/421/0007413124',\n",
       " 'https://n.news.naver.com/mnews/article/003/0012428296',\n",
       " 'https://n.news.naver.com/mnews/article/016/0002280804',\n",
       " 'https://n.news.naver.com/mnews/article/277/0005392558',\n",
       " 'https://n.news.naver.com/mnews/article/003/0012428375',\n",
       " 'https://n.news.naver.com/mnews/article/277/0005392570',\n",
       " 'https://n.news.naver.com/mnews/article/003/0012428399',\n",
       " 'https://n.news.naver.com/mnews/article/015/0004960267',\n",
       " 'https://n.news.naver.com/mnews/article/011/0004313593',\n",
       " 'https://n.news.naver.com/mnews/article/022/0003914302']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(URLs), len(Visited), len(Skipped), Visited[-1], #URLs[-1]\n",
    "Visited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[93], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m     Skipped\u001b[38;5;241m.\u001b[39mappend(url[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlink\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlink\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43muser-agent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mua\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m Visited\u001b[38;5;241m.\u001b[39mappend(url[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlink\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resp \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[7], line 9\u001b[0m, in \u001b[0;36mdownload\u001b[1;34m(url, params, data, headers, method, retries)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdownload\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m{}, data\u001b[38;5;241m=\u001b[39m{}, headers\u001b[38;5;241m=\u001b[39m{}, method \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGET\u001b[39m\u001b[38;5;124m'\u001b[39m, retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m):\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# if not canFetch(url, url):  # robots.txt 검사\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m#     print('수집하면 안됨')  # False일때\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# parameter로 받은 친구들을 request 옵션으로 넣어서 request 객체 생성\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\SAMSUNG\\anaconda3\\envs\\KoreaUniv\\lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\SAMSUNG\\anaconda3\\envs\\KoreaUniv\\lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\SAMSUNG\\anaconda3\\envs\\KoreaUniv\\lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32mc:\\Users\\SAMSUNG\\anaconda3\\envs\\KoreaUniv\\lib\\site-packages\\requests\\adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    483\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[1;32mc:\\Users\\SAMSUNG\\anaconda3\\envs\\KoreaUniv\\lib\\site-packages\\urllib3\\connectionpool.py:790\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    787\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    789\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 790\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    791\u001b[0m     conn,\n\u001b[0;32m    792\u001b[0m     method,\n\u001b[0;32m    793\u001b[0m     url,\n\u001b[0;32m    794\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    795\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    796\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    797\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    798\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[0;32m    799\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[0;32m    800\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    801\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    802\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[0;32m    803\u001b[0m )\n\u001b[0;32m    805\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[0;32m    806\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\SAMSUNG\\anaconda3\\envs\\KoreaUniv\\lib\\site-packages\\urllib3\\connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    534\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 536\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[1;32mc:\\Users\\SAMSUNG\\anaconda3\\envs\\KoreaUniv\\lib\\site-packages\\urllib3\\connection.py:461\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    458\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[0;32m    460\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[1;32m--> 461\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    464\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[1;32mc:\\Users\\SAMSUNG\\anaconda3\\envs\\KoreaUniv\\lib\\http\\client.py:1377\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1375\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1376\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1377\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1378\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[0;32m   1379\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\SAMSUNG\\anaconda3\\envs\\KoreaUniv\\lib\\http\\client.py:320\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 320\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    321\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[0;32m    322\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\SAMSUNG\\anaconda3\\envs\\KoreaUniv\\lib\\http\\client.py:281\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 281\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    282\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[0;32m    283\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\SAMSUNG\\anaconda3\\envs\\KoreaUniv\\lib\\socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    703\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 704\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    706\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\SAMSUNG\\anaconda3\\envs\\KoreaUniv\\lib\\ssl.py:1275\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1271\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1272\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1273\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1274\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1275\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1276\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mc:\\Users\\SAMSUNG\\anaconda3\\envs\\KoreaUniv\\lib\\ssl.py:1133\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1132\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1133\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1134\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1135\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# news 본문만 저장\n",
    "\n",
    "URLs = []\n",
    "Visited = []\n",
    "Skipped = []\n",
    "\n",
    "optin = ['news.naver.com', 'n.news.naver.com', 'imgnews.pstatic.net']  # news만\n",
    "optout = []\n",
    "\n",
    "ua = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36'\n",
    "\n",
    "depth = 5   \n",
    "\n",
    "# URLs.append({'link':'https://search.naver.com/search.naver?where=nexearch&sm=top_hty&fbm=0&ie=utf8&query=수지', 'depth': 0})\n",
    "# URLs.append('https://www.google.com/search?q=수지')\n",
    "# URLs.append({'link':'https://search.daum.net/search?w=tot&DA=YZR&t__nil_searchbox=btn&q=수지', 'depth': 0}) # fix\n",
    "URLs.append({'link':'https://news.naver.com/', 'depth': 0})\n",
    "\n",
    "NEWS_TEXT = './naver_news'\n",
    "NEWS_IMG = './naver_img'\n",
    "\n",
    "while URLs:\n",
    "    url = URLs.pop(-1) \n",
    "    \n",
    "    if url['depth'] > depth: \n",
    "        Skipped.append(url['link'])\n",
    "        continue\n",
    "\n",
    "    resp = download(url['link'], headers={'user-agent': ua})\n",
    "\n",
    "    Visited.append(url['link'])\n",
    "    \n",
    "    if resp is None:\n",
    "        continue\n",
    "\n",
    "    if re.search(r'image\\/(\\w+);?', resp.headers['content-type']):\n",
    "        ext = re.search(r'image\\/(\\w+);?', resp.headers['content-type']).group(1)\n",
    "        fname = re.search(r'/(\\w+\\.jpg|jpeg|png|bmp|gif)', url['link'])\n",
    "        if fname:    \n",
    "            with open(NEWS_IMG+'/'+fname.group(1), 'wb') as fp:\n",
    "                fp.write(resp.content)\n",
    "            # binary 값이므로 resp.content\n",
    "        else:\n",
    "            print(url['link'], resp.headers, ext)\n",
    "    if re.search(r'text\\/html', resp.headers['content-type']):\n",
    "        dom = BeautifulSoup(resp.text, 'html.parser')\n",
    "        # 3. 영역제한 - 뉴스만\n",
    "\n",
    "        # 뉴스인 경우\n",
    "        # 3-1. 메뉴\n",
    "        linksA = dom.select('ul.Nlnb_menu_list li > a') # ul 클래스 가진 li의 자식인 a를 찾음  # fix\n",
    "        # 위의 부모가 .sa|ss_text > a:has(> strong) sa혹은 ss_text 클래스가 a를 가지는데 이 a는 strong을 가진다\n",
    "        # 3-2.뉴스\n",
    "        linksB = dom.select('div[class$=\"_text\"] > a[href]:has(> strong)')   # fix\n",
    "\n",
    "        # 3-3. 뉴스 본문\n",
    "        linksC = dom.select('#dic_area a[href], #dic_area img[data-src]')   # 여기 바꿈 naver는 data-src이기 떄문에\n",
    "        article = dom.select_one('#dic_area')\n",
    "        # 뉴스 본문을 찾기 위해 특정 element만\n",
    "\n",
    "        if article:\n",
    "            with open(NEWS_TEXT+'/'+re.search(r'(\\d{10})$',url['link']).group(1)+'.txt', 'w', encoding='utf8') as fp:  # 기사 number로 text 파일 이름 줄것임\n",
    "                fp.write(article.get_text())\n",
    "        # 저장 file\n",
    "\n",
    "        for link in linksA+linksB+linksC:  # fix\n",
    "            if link.has_attr('href'):\n",
    "                newURL = link.attrs['href']\n",
    "            elif link.has_attr('src'):\n",
    "                newURL = link.attrs['src']\n",
    "            elif link.has_attr('data-src'):\n",
    "                newURL = link.attrs['data-src']\n",
    "\n",
    "            if not re.match(r'#|javascript|mailto|data', newURL):\n",
    "                noremlizedURL = urljoin(url['link'], newURL)\n",
    "                if urlparse(noremlizedURL).netloc not in optin:\n",
    "                    continue\n",
    "                \n",
    "                # if urlparse(noremlizedURL).netloc in optout:\n",
    "                #     continue\n",
    "                if noremlizedURL not in Visited and noremlizedURL not in [url['link'] for url in URLs] and noremlizedURL not in Skipped: # fix\n",
    "                    URLs.append({'link':noremlizedURL, 'depth': url['depth']+1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 10), match='image/jpeg'>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp = download('https://imgnews.pstatic.net/image/030/2024/03/15/0003189376_001_20240315091501054.jpg?type=w647')\n",
    "re.search(r'image\\/(\\w+);?', resp.headers['content-type'])\n",
    "# jpg 맞는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('0003189376_001_20240315091501054.jpg',)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search(r'/(\\w+\\.jpg|jpeg|png|bmp|gif)', 'https://imgnews.pstatic.net/image/030/2024/03/15/0003189376_001_20240315091501054.jpg?type=w647').groups(1)\n",
    "# 파일 이름 구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0003189376_001_20240315091501054.jpg\n",
      "https://imgnews.pstatic.net/image/277/2024/03/15/0005392596_001_20240315102601385.jpg?type=w647 {'Accept-Ranges': 'bytes', 'Content-Length': '198962', 'Content-Type': 'image/jpeg', 'Last-Modified': 'Fri, 15 Mar 2024 01:26:19 GMT', 'P3p': 'CP=\"ALL CURa ADMa DEVa TAIa OUR BUS IND PHY ONL UNI PUR FIN COM NAV INT DEM CNT STA POL HEA PRE LOC OTC\"', 'Cache-Control': 'max-age=6702', 'Expires': 'Fri, 15 Mar 2024 05:19:38 GMT', 'Date': 'Fri, 15 Mar 2024 03:27:56 GMT', 'Connection': 'keep-alive'}\n"
     ]
    }
   ],
   "source": [
    "fname = re.search(r'/(\\w+\\.jpg|jpeg|png|bmp|gif)', 'https://imgnews.pstatic.net/image/030/2024/03/15/0003189376_001_20240315091501054.jpg?type=w647').group(1)\n",
    "print(fname)\n",
    "resp = download(url['link'], headers={'user-agent': ua})\n",
    "print(url['link'], resp.headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crawling이랑 scraping이랑 다르다\n",
    "# 많이들 혼동해서 사용한다\n",
    "# 하지만 분명히 다르다\n",
    "# crawling - 전체 html 파일이 필요한다 / large scale\n",
    "# scraping - 특정 url에서 링크 가져오고 이름 가져오고 등등 / not large scale - 즉, 내가 원하는 부분만 가능 / 원할때 그만할 수 있음\n",
    "# focused - large scale / 끝없이 나오기 때문에\n",
    "# 이제는 scraping을 사용할 것이다"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KoreaUniv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
