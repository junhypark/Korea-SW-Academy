{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 띄어쓰기 ngram\n",
    "# q = 'ABCDE..'\n",
    "# i = ^\n",
    "# r = ''\n",
    "\n",
    "# 2. gram\n",
    "# i가 0일때 q의 index 0 이 나오는지 확인하고\n",
    "# 해당 맞는 애들을 넣는다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "\n",
    "def fileids(path):\n",
    "    return list(map(lambda f:path + ('' if path[-1] == '/' else '/')+f, listdir(path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 띄어쓰기\n",
    "def ngram(s, n=2, t=True): # t=True;어절, False;음절\n",
    "    result = []\n",
    "    \n",
    "    if not t:\n",
    "        s = list(s)\n",
    "        \n",
    "    for i in range(len(s)-(n-1)):\n",
    "        result.append(tuple(s[i:i+n])) # 튜플로 수정\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findKey(gram, key):\n",
    "    k = tuple(key)\n",
    "    return list(filter(lambda g:g[:len(k)] == k, gram.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoSpacing(q, n=2):\n",
    "    i = 0\n",
    "    while i < len(q)-(n-1):\n",
    "        i += 1\n",
    "\n",
    "    return r\n",
    "\n",
    "# autoSpacing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# gram = {2:[],3,4,5}\n",
    "corpus = list()\n",
    "for f in fileids('news'):\n",
    "    with open(f, 'r', encoding='utf8') as fp:\n",
    "        corpus.append(\n",
    "            re.sub(r'^\\s+|\\s+$', '',\n",
    "                   re.sub(r'\\s+', ' ',\n",
    "                          re.sub(r'\\sCopyright.+', ' ',\n",
    "                                 re.sub(r'[\\xa0-\\xff]', ' ', fp.read())))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "gram = {2:list(), 3:list(), 4:[], 5:[]}\n",
    "for c in corpus:\n",
    "    for s in sent_tokenize(c):\n",
    "        gram[2].extend(ngram(s, 2, False))\n",
    "        gram[3].extend(ngram(s, 3, False))\n",
    "        gram[4].extend(ngram(s, 4, False))\n",
    "        gram[5].extend(ngram(s, 5, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "gram = {2:Counter(gram[2]), 3:Counter(gram[3]), 4:Counter(gram[4]), 5:Counter(gram[5])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoSpacing(q, n=2):\n",
    "    i = 0\n",
    "    r = list()\n",
    "\n",
    "    r.append(q[i:i+(n-1)])\n",
    "\n",
    "    while i < len(q)-(n-1):\n",
    "        k = r[-(n-1):]\n",
    "        keys = findKey(gram[n], k)\n",
    "        candidates = {j:gram[n][j] for j in keys}\n",
    "        best = sorted(candidates.items(), key = lambda r:r[1], revese=True)[0]\n",
    "        if best[0][-1] == ' ':\n",
    "            r.append(' ')\n",
    "        else:\n",
    "            r.append(q[i+(n-1)])\n",
    "            i += 1\n",
    "\n",
    "    return ''.join(r)\n",
    "\n",
    "autoSpacing(sent_tokenize(corpus[0])[0].replace(' ',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "findKey(ngram[2], '초')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Branch Entropy\n",
    "# -sum(p*log(p))\n",
    "# 통계적 지식 없이 어감 분류\n",
    "\n",
    "tokens = Counter('\\n'.join(corpus).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "from collections import defaultdict\n",
    "\n",
    "def stemming(q):\n",
    "    result = list()\n",
    "    for i in range(len(q)):\n",
    "        candidates = defaultdict(lambda:0)\n",
    "        given = sum([tokens[k] for k in tokens.keys() if re.match(q[:i+1], k)])\n",
    "        p = [tokens[k]/given for k in tokens.keys() if re.match(q[:i+2], k)]\n",
    "        result.append(-sum([cp*log(cp) for cp in p]))\n",
    "\n",
    "    return result\n",
    "\n",
    "stemming('실행하고')\n",
    "\n",
    "# 이런거 할 때는 작은 데이터셋으로 먼저 해보고\n",
    "# 큰 데이터 셋으로 진행하는게 좋아보인다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perplexity\n",
    "\n",
    "# 응집력을 뜻함\n",
    "# clustering 할 때 perplexity로 평가를 한다\n",
    "# perplexity가 크면 응집력이 쎄다\n",
    "# 여기서 값이 많이 올라가는 부분을 찾아서 오류를 검출 가능하다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 음절 ㅠp(xt+1|x1,...,xt)\n",
    "# (4)   ==> sqrt(1/(p(x2|x1)*p(x3|x1,x2)*p(x4|x1,x2,x3)), 4) (1/4 제곱근)\n",
    "#                   p(x1,x2) / x1 * p(x1,x2,x3) / x1,x2 * p(x1,x2,x3,x4) / x1,x2,x3 = p(x1,x2,x3,x4) / p(x1) = sqrt(freq(x1,x2,x3,x4) / freq(x1))\n",
    "# 응집력에 대한 값 = cohesion score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "def cohension(t):\n",
    "    t1 = t[:1]\n",
    "    freq_t1 = sum([tokens[x] for x in list(filter(lambda k: re.match(t1, k), tokens.keys()))])\n",
    "    print(t1, freq_t1)\n",
    "\n",
    "    threshold = 0.0\n",
    "\n",
    "    for i in range(1, len(t)):\n",
    "        t2 = t[:i+1]\n",
    "        freq_t2 = sum([tokens[x] for x in list(filter(lambda k: re.match(t2, k), tokens.keys()))])\n",
    "        score = (freq_t2/freq_t1)**(1/len(t2))\n",
    "        # print(t2, freq_t1/freq_t2)\n",
    "        if score < threshold:\n",
    "            print(i)\n",
    "        threshold = score\n",
    "    return (t,) if i == len(t) else (t[:i], t[i:])\n",
    "\n",
    "cohension('금융감독원')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizing\n",
    "# sent_tokenize, word_tokenize, regex_tokenize, TweetTokinize <= 언어적x, 구두점 (문장기호)로 잘라내고 나눠냄\n",
    "# 한국어 특징 반영하려함 => 형태소 분석기를 사용하여 혹은 명사 추출기, 품사 부착기를 사용하여 반영 시도\n",
    "# 언어적 특징 이용 => ngram\n",
    "# 통계적 방법 => branch-entropy, perplexity => 응집력에 대한 것들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# byte pair encoding\n",
    "# 분적을 생성 사능하고 패턴을 만들어서 불용어 처리를 할 수 있다\n",
    "# * 불용어 처리란? 토큰으로 삼아도 아무 제공을 못하는 애들, 의미전달을 못하는 애들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.open(stopwords.fileids()[8]).read().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization\n",
    "# 일반화 정규화\n",
    "# 앞의 과정들은 feature 추출\n",
    "# 이때 normalization은 각 단어들을 하나의 format으로 만들어주는 과정이다\n",
    "# 이때, - 혹은 ' 과 같은 애들로 인해 단어의 의미가 변절될 우려가 생긴다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.open('english').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from string import punctuation\n",
    "\n",
    "# normalization 단계\n",
    "# 1. 대소문자 일치\n",
    "# 2. 문장 기호 어떻게 처리할지 = 여기서는 삭제 해보자\n",
    "# 3. 불용어 처리 => 선택적으로 그리고 전략적으로\n",
    "# 도메인에 맞는 단어와 내용에 맞게 불용어 처리를 해야한다\n",
    "\n",
    "d = \"To be or not to be.\"\n",
    "r = []\n",
    "for t in re.sub(f'[{punctuation}]', '',  d.lower()).split():\n",
    "    if t not in stop:\n",
    "        r.append(t)\n",
    "r"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KoreaUniv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
